# M3. MLOps 파이프라인 개발 (CT)

## 3.1 데이터 추출

### Airflow 추가 설정

- Admin → Connection에서 MySQL 커넥션 추가
	- Connection Id : `feature_store`
	- Connection Type : `MySQL`
	- Host : `0.0.0.0`
	- Schema : `mlops`
	- Login : `root`
	- Password : `root`
	- Port : `3306`

### 데이터 병합

- `credit_score` 테이블과 `credit_score_feature` 테이블을 병합해서 타겟값까지 포함하고 있는 테이블 생성
	- 최근 일주일 데이터만 포함하고 있는 테이블로 관리할 예정
		- 오늘 날짜의 데이터가 있다면 해당 데이터를 지우고 다시 추가
	- 아래와 같이 날짜를 Jinja template 으로 작성하는 쿼리 사용

```sql
-- pipelines/continuous_training/data_extract/features.sql
-- 1. 일주일 전 날짜 이전 데이터 삭제
DELETE FROM mlops.credit_score_features_target
WHERE base_dt <= DATE_FORMAT(
        DATE_ADD(
            STR_TO_DATE(
                '{{ data_interval_start.in_timezone("Asia/Seoul").to_date_string() }}',
                '%Y-%m-%d'
            ),
            INTERVAL -7 DAY
        ),
        '%Y-%m-%d'
    )
    OR base_dt = STR_TO_DATE(
        '{{ data_interval_start.in_timezone("Asia/Seoul").to_date_string() }}',
        '%Y-%m-%d'
    );
-- 2. 새로운 데이터 삽입
INSERT INTO mlops.credit_score_features_target (
        base_dt,
        id,
        customer_id,
        date,
        age,
        occupation,
        annual_income,
        monthly_inhand_salary,
        num_bank_accounts,
        num_credit_card,
        interest_rate,
        num_of_loan,
        type_of_loan,
        delay_from_due_date,
        num_of_delayed_payment,
        changed_credit_limit,
        num_credit_inquiries,
        credit_mix,
        outstanding_debt,
        credit_utilization_ratio,
        credit_history_age,
        payment_of_min_amount,
        total_emi_per_month,
        amount_invested_monthly,
        payment_behaviour,
        monthly_balance,
        credit_score
    )
SELECT STR_TO_DATE(
        '{{ data_interval_start.in_timezone("Asia/Seoul").to_date_string() }}',
        '%Y-%m-%d'
    ) AS base_dt,
    b.id,
    b.customer_id,
    b.date,
    a.age,
    a.occupation,
    a.annual_income,
    a.monthly_inhand_salary,
    a.num_bank_accounts,
    a.num_credit_card,
    a.interest_rate,
    a.num_of_loan,
    a.type_of_loan,
    a.delay_from_due_date,
    a.num_of_delayed_payment,
    a.changed_credit_limit,
    a.num_credit_inquiries,
    a.credit_mix,
    a.outstanding_debt,
    a.credit_utilization_ratio,
    a.credit_history_age,
    a.payment_of_min_amount,
    a.total_emi_per_month,
    a.amount_invested_monthly,
    a.payment_behaviour,
    a.monthly_balance,
    b.credit_score
FROM mlops.credit_score_features a
    INNER JOIN (
        SELECT *
        FROM mlops.credit_score
        WHERE date BETWEEN DATE_ADD(
                STR_TO_DATE(
                    '{{ data_interval_start.in_timezone("Asia/Seoul").to_date_string() }}',
                    '%Y-%m-%d'
                ),
                INTERVAL -1 MONTH
            )
            AND STR_TO_DATE(
                '{{ data_interval_start.in_timezone("Asia/Seoul").to_date_string() }}',
                '%Y-%m-%d'
            )
    ) b ON a.id = b.id
    AND a.customer_id = b.customer_id;
```

### DAG 개발

- DAG 개발 순서는 다음과 같음
	- `EmptyOperator` 이용해서 빈 Task로 큰 틀을 구성
	- 각 Task를 추가 작성
	- 추가 작성 후 테스트
	- 문제 없으면 다음 Task 개발

```python
# pipelines/continuous_training/continuous_training_dag.py

from datetime import datetime

import pendulum
from airflow.providers.standard.operators.empty import EmptyOperator
from airflow.sdk import DAG

from utils.callbacks import failure_callback, success_callback

local_timezone = pendulum.timezone("Asia/Seoul")

with DAG(
    dag_id="credit_score_classification_ct",
    default_args={
        "owner": "user",
        "depends_on_past": False,
        "email": ["otzslayer@gmail.com"],
        "on_failure_callback": failure_callback,
        "on_success_callback": success_callback,
    },
    description="A DAG for continuous training",
    schedule=None,
    start_date=datetime(2025, 1, 1, tzinfo=local_timezone),
    catchup=False,
    tags=set(["lgcns", "mlops"]),
) as dag:
    data_extract = EmptyOperator(task_id="data_extraction")

    data_preprocessing = EmptyOperator(task_id="data_preprocessing")

    training = EmptyOperator(task_id="model_training")

    data_extract >> data_preprocessing >> training

```

- 이후 아래와 같이 데이터 추출 관련 코드 작성 
	- SQL 쿼리를 실행하는 `SQLExecuteQueryOperator`를 사용

```python
# pipelines/continuous_training/continuous_training_dag.py

import os
from datetime import datetime

import pendulum
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.standard.operators.empty import EmptyOperator
from airflow.sdk import DAG, Variable

from utils.callbacks import failure_callback, success_callback
from utils.common import read_sql_file

local_timezone = pendulum.timezone("Asia/Seoul")
conn_id = "feature_store"
airflow_dags_path = Variable.get("AIRFLOW_DAGS_PATH")
sql_file_path = os.path.join(
    airflow_dags_path,
    "pipelines",
    "continuous_training",
    "data_extract",
    "features.sql",
)

with DAG(
    dag_id="credit_score_classification_ct",
    default_args={
        "owner": "user",
        "depends_on_past": False,
        "email": ["otzslayer@gmail.com"],
        "on_failure_callback": failure_callback,
        "on_success_callback": success_callback,
    },
    description="A DAG for continuous training",
    schedule=None,
    start_date=datetime(2025, 1, 1, tzinfo=local_timezone),
    catchup=False,
    tags=set(["lgcns", "mlops"]),
) as dag:

    data_extract = SQLExecuteQueryOperator(
        task_id="data_extraction",
        conn_id=conn_id,
        sql=read_sql_file(sql_file_path),
        split_statements=True,
    )

    data_preprocessing = EmptyOperator(task_id="data_preprocessing")

    training = EmptyOperator(task_id="model_training")

    data_extract >> data_preprocessing >> training

```

- `SQLExecuteQueryOperator` 에 사용되는 파라미터

| **파라미터**     | **타입**         | **설명**                                                                            |
| ---------------- | ---------------- | ----------------------------------------------------------------------------------- |
| `task_id`          | `str`              | DAG에서 사용될 Task의 ID                                                          |
| `conn_id`          | `str`              | Airflow에서 정의한 **연결 ID** (Airflow UI > Admin > Connections에서 확인 가능)     |
| `sql`              | `str` or `list`      | 실행할 SQL 쿼리 (문자열 또는 SQL 파일 경로 가능)                                    |
| `parameters`       | `dict` or `iterable` | SQL에 전달할 파라미터 (sqlalchemy.text 스타일 사용 가능)                            |
| `autocommit`       | `bool`             | 트랜잭션을 자동으로 커밋할지 여부 (기본값: False)                                   |
| `split_statements` | `bool`             | 여러 개의 SQL 문장을 하나의 리스트로 제공할 경우 **분할 실행 여부** (기본값: False) |
| `return_last`      | `bool`             | XCom에 저장할 때 마지막 쿼리의 결과만 저장할지 여부 (기본값: True)                  |
| `handler`          | `callable`         | 쿼리 결과를 가공할 핸들러 함수 (예: lambda cursor: cursor.fetchall())               |
| `database`         | `str`              | 특정 데이터베이스를 명시할 경우 사용 (MySQL, Postgres 등에서 지원)                  |

## 3.2 데이터 전처리

### 전처리 클래스 개발 로직

1. 데이터 불러오기
	- 만약 불러온 데이터가 비어있다면 오류 발생
2. 수치형 변수에 대해서 `RobustScaler` 적용
	- 추후 서빙 시 각 변수에 대한 `RobustScaler`를 불러와서 적용해야 하기 떄문에 객체 덤프 필요
		- 아티팩트 폴더에 저장
	- 학습/검증 데이터에 대해 적합시키고 변환 수행
3. 변환한 학습/검증 데이터를 아티팩트 폴더에 저장 
4. 파일 실행 부분에서 모델 이름과 실행 날짜를 인자로 받아서 아티팩트 폴더 아래에 관리

```python
# pipelines/continuous_training/data_preprocessing/preprocessor.py

import os
from typing import Optional

import joblib
import numpy.typing as npt
import pandas as pd
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sqlalchemy import create_engine

from utils.dates import DateValues

# .env 파일 로드
load_dotenv()

feature_store_url = os.getenv("FEATURE_STORE_URL")
artifacts_path = os.getenv("ARTIFACTS_PATH")

TARGET_NAME = "credit_score"


class Preprocessor:
    """전처리 클래스

    Args:
        model_name (str): 모델명
            해당 이름으로 아티팩트 폴더 아래 관련 객체들이 저장됩니다.
        base_dt (str, optional): 해당 값이 없는 경우 오늘 날짜로 대체됩니다.
    """

    __ROBUST_SCALING_FEATURES = [
        "age",
        "annual_income",
        "monthly_inhand_salary",
        "num_bank_accounts",
        "num_credit_card",
        "interest_rate",
        "num_of_loan",
        "delay_from_due_date",
        "num_of_delayed_payment",
        "changed_credit_limit",
        "num_credit_inquiries",
        "outstanding_debt",
        "credit_utilization_ratio",
        "credit_history_age",
        "total_emi_per_month",
        "amount_invested_monthly",
        "monthly_balance",
    ]

    def __init__(
        self,
        model_name: str,
        base_dt: str = DateValues.get_current_date(),
    ):
        self._model_name = model_name
        self._base_dt = base_dt
        self._save_path = os.path.join(
            artifacts_path,
            "preprocessing",
            self._model_name,
            self._base_dt,
        )
        self._encoder_path = os.path.join(self._save_path, "encoders")
        self._make_dirs()

    def transform(self):
        data = self._fetch_data()
        x_train, y_train, x_val, y_val = self._train_val_split(data=data)
        x_train, x_val = self._transform_with_robust_scaler(
            x_train=x_train, x_val=x_val
        )
        self._save_preprocessed_data(
            feature=x_train, target=y_train, is_train=True
        )
        self._save_preprocessed_data(
            feature=x_val, target=y_val, is_train=False
        )

    def _fetch_data(self) -> pd.DataFrame:
        """`Preprocessor`를 초기화할 때 입력값으로 받은 `base_dt`를 기준으로 데이터를 불러옵니다.
        데이터가 한 건도 존재하지 않는 경우 `ValueError`가 발생합니다.

        Raises:
            ValueError: 데이터가 한 건도 없을 때 발생

        Returns:
            pd.DataFrame: 불러온 데이터
        """

        engine = create_engine(feature_store_url)

        q = f"""
            select *
            from mlops.credit_score_features_target
            where base_dt = '{self._base_dt}'
        """

        with engine.connect() as conn:
            data = pd.read_sql(q, con=conn.connection)

        if data.empty:
            raise ValueError("Fetched data is empty! :(")

        return data

    def _train_val_split(
        self,
        data: pd.DataFrame,
        val_size: Optional[float] = 0.3,
        random_state: Optional[int] = 42,
    ) -> tuple[pd.DataFrame, npt.NDArray, pd.DataFrame, npt.NDArray]:
        """데이터를 학습/검증 그리고 피처, 타겟으로 나눕니다.

        Args:
            data (pd.DataFrame): 데이터
            val_size (Optional[float], optional): 검증 데이터 비율
                Defaults to 0.3.
            random_state (Optional[int], optional): 랜덤 시드
                Defaults to 42.

        Returns:
            tuple[pd.DataFrame, npt.NDArray, pd.DataFrame, npt.NDArray]:
                학습 피처, 학습 타겟, 검증 피처, 검증 타겟
        """
        train, val = train_test_split(
            data, test_size=val_size, random_state=random_state
        )

        x_train = train.drop([TARGET_NAME], axis=1)
        y_train = train[TARGET_NAME].to_numpy()

        x_val = val.drop([TARGET_NAME], axis=1)
        y_val = val[TARGET_NAME].to_numpy()

        return x_train, y_train, x_val, y_val

    def _transform_with_robust_scaler(
        self,
        x_train: pd.DataFrame,
        x_val: pd.DataFrame,
        features: list[str] = __ROBUST_SCALING_FEATURES,
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """RobustScaler를 이용해서 수치형 변수를 스케일링합니다.

        Args:
            data (pd.DataFrame): 데이터
            features (list[str], optional): 대상 피처
                Defaults to `self.__ROBUST_SCALING_FEATURES`.

        Returns:
            tuple[pd.DataFrame, pd.DataFrame]: 스케일링 완료 후 데이터 (학습, 검증)
        """
        robust_scalers = {}

        for feature in features:
            scaler = RobustScaler()
            robust_scalers[feature] = scaler.fit(x_train[[feature]])
            x_train[feature] = scaler.transform(x_train[[feature]])
            x_val[feature] = scaler.transform(x_val[[feature]])
            print(f"RobustScaler has been applied to {feature}.")

        joblib.dump(
            robust_scalers,
            os.path.join(self._encoder_path, "robust_scaler.joblib"),
        )

        return x_train, x_val

    def _make_dirs(self) -> None:
        """저장될 경로가 존재하지 않으면 해당 폴더를 생성합니다."""
        if not os.path.isdir(self._encoder_path):
            os.makedirs(self._encoder_path)

    def _save_preprocessed_data(
        self,
        feature: pd.DataFrame,
        target: npt.NDArray,
        is_train: Optional[bool] = True,
    ) -> None:
        """전처리된 데이터를 저장합니다.
        입력으로 받은 피처와 타겟값을 다시 합치고, 학습/검증 데이터에 따라 다른 이름으로 저장합니다.

        Args:
            feature (pd.DataFrame): 피처 데이터
            target (npt.NDArray): 타겟값
            is_train (Optional[bool], optional): 학습 데이터 여부
                Defaults to True.
        """
        file_name = f"{self._model_name}_{'train' if is_train else 'val'}.csv"
        data = feature.copy()
        data[TARGET_NAME] = target

        data.to_csv(os.path.join(self._save_path, file_name), index=False)

    @staticmethod
    def _rename_columns_to_lowercase(data: pd.DataFrame) -> pd.DataFrame:
        """컬럼명을 소문자로 변환한 후 해당 데이터프레임을 반환합니다.

        Args:
            data (pd.DataFrame): 원본 데이터

        Returns:
            pd.DataFrame: 컬럼명을 소문자로 변환한 데이터
        """

        data.columns = [col.lower() for col in data.columns]

        return data


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="An argument parser for preprocessor."
    )

    parser.add_argument(
        "--model_name",
        type=str,
        default="credit_score_classification",
        dest="model_name",
    )
    parser.add_argument(
        "--base_dt",
        type=str,
        default=DateValues.get_current_date(),
        dest="base_dt",
    )

    args = parser.parse_args()

    preprocessor = Preprocessor(
        model_name=args.model_name, base_dt=args.base_dt
    )
    preprocessor.transform()

```

### 환경 변수 관리 방안

- 개발 환경에서는 프로젝트 루트 폴더에 `.env` 파일 생성해서 환경 변수 작성해 쉽게 관리 가능
	- `python-dotenv` 라이브러리의 `load_dotenv()` 함수 사용
	- 운영 환경에서는 보안에 취약하기 떄문에 다른 방법을 사용해야 함
		- 특히 `.env` 파일을 Git에서 추적하게 해놓으면 큰 문제가 생길 수 있음
- 다른 방법
	- Airflow를 사용하는 경우에는 Variables나 Secrets에서 관리할 수 있음
	- `sudo` 권한이 있는 경우 `/etc/environment` 에 추가하여 시스템 환경변수로 추가할 수 있음
	- Pydantic을 활용하여 환경 설정하는 것이 바람직함

- 개발 용도로 사용하기 위해 `.env` 생성

```shell
FEATURE_STORE_URL=mysql+pymysql://root:root@localhost:3306/mlops
ARTIFACTS_PATH=/home/codespace/airflow/artifacts
```

> [!tip]
> - Python으로 개발한 애플리케이션에서 환경 설정을 할 때 가장 널리 사용되는 라이브러리는 `pydantic-settings`
> 	- Pydantic 기반의 환경 설정 라이브러리
> 	- 장점이 많음
> 		- 자동 타입 변환 및 유효성 검사 가능
> 		- 명확한 우선 순위를 갖고 있음
> 		    - 직접 코드에서 설정 > 환경 변수 > `.env` 파일 > 모델의 기본값
> 		- 중앙화된 관리 가능
> 		- IDE 자동 완성 및 타입 힌팅 가능
> 		- 시크릿 관리 기능 내장
> 		    - `secrets_dir` 인자 사용

### Docker 설정파일 개발

- `pipelines/continuous_training` 디렉토리에 `docker` 라는 폴더로 이동
	- 폴더 안에 `requirements.txt`, `Dockerfile`, `docker-compose.yml` 파일 수정
	- 이 파일들로 전처리, 학습 모듈을 모두 띄울 수 있음

```txt
# pipelines/continuous_training/docker/requirements.txt

joblib==1.5.1
numpy==2.3.2
pandas==2.3.1
python-dotenv==1.1.1
pymysql==1.1.1
scikit-learn==1.7.1
SQLAlchemy==1.4.54
mlflow==3.2.0
bentoml==1.4.19
catboost==1.2.8
pip==25.2
tqdm==4.67.1

```

```dockerfile
# pipelines/continuous_training/docker/Dockerfile

FROM python:3.12-slim-bookworm
LABEL maintainer="otzslayer@gmail.com"
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

ARG USER_HOME=/home/codespace
ARG UTIL_PATH=utils
ARG PREPROCESSING_PATH=pipelines/continuous_training/data_preprocessing
ARG REQUIREMENTS_PATH=pipelines/continuous_training/docker

RUN groupadd --gid 1000 codespace \
    && useradd --uid 1000 --gid codespace --shell /bin/bash --create-home codespace

COPY --chown=codespace:codespace ${UTIL_PATH}/ ${USER_HOME}/utils
COPY --chown=codespace:codespace ${PREPROCESSING_PATH}/preprocessor.py \
    ${USER_HOME}/data_preprocessing/
COPY --chown=codespace:codespace ${REQUIREMENTS_PATH}/requirements.txt \
    ${USER_HOME}/

USER codespace
WORKDIR ${USER_HOME}

ENV PYTHONUNBUFFERED=1 \
    VIRTUAL_ENV="${USER_HOME}/.venv"

ENV PATH="${VIRTUAL_ENV}/bin:$PATH"

RUN mkdir -p ${USER_HOME}/artifacts \
    && uv init --python 3.12 \
    && uv venv --python 3.12 --seed \
    && uv pip install -r ${USER_HOME}/requirements.txt

```

```yaml
# pipelines/continuous_training/docker/docker-compose.yml

services:
  continuous_training_pipeline:
    build:
      context: ../../..
      dockerfile: pipelines/continuous_training/docker/Dockerfile
    image: credit_score_classification:ct-pipeline-latest
    container_name: credit_score_classification_ct_pipeline
    volumes:
      - ${HOME}/airflow/artifacts:/home/codespace/artifacts
    environment:
      PYTHONPATH: /home/codespace
      ARTIFACTS_PATH: /home/codespace/artifacts
      FEATURE_STORE_URL: mysql+pymysql://root:root@mariadb:3306/mlops
    command: >
      python ${PYTHON_FILE} --model_name ${MODEL_NAME} --base_dt ${BASE_DT}
    networks:
      mlops_network:
networks:
  mlops_network:
    name: mlops_network
    external: true

```

### DAG 개발

```python
# pipelines/continuous_training/continuous_training_dag.py

...

from airflow.providers.standard.operators.bash import BashOperator

...

kst_ds_template = (
    "{{ data_interval_start.in_timezone('Asia/Seoul').to_date_string() }}"
)

...

data_preprocessing = BashOperator(
	task_id="data_preprocessing",
	bash_command=f"cd {airflow_dags_path}/pipelines/continuous_training/docker &&"
	"docker compose up --build && docker compose down",
	env={
		"PYTHON_FILE": "/home/codespace/data_preprocessing/preprocessor.py",
		"MODEL_NAME": "credit_score_classification",
		"BASE_DT": kst_ds_template,
	},
	append_env=True,
	retries=1,
)

...
```

- DAG 실행 전 관련 폴더를 미리 생성
	- 미리 생성하지 않고 DAG를 실행하면 `root` 권한으로 폴더가 생성되어 권한 문제가 발생함

```shell
mkdir ~/airflow/artifacts
mkdir ~/bentoml
mkdir ~/mlruns
```

## 3.3 모델 학습/평가

### 모델 학습/평가 클래스 개발 로직

1. 데이터 불러오기
	- 불러온 후 학습 데이터와 검증 데이터에서 피처와 타겟값을 분리
	- CatBoost에서 사용 가능한 형태로 변환 `(catboost.Pool)`
2. 하이퍼파라미터 튜닝
	- CatBoost 모델을 학습하며, Grid Search로 최적의 하이퍼파라미터 탐색
3. 학습 결과를 MLflow와 아티팩트 폴더에 저장
	- 튜닝 중 매 시행에 대한 메타데이터를 MLflow에 저장하고 모델을 아티팩트 폴더에 저장
4. 최적 모델을 저장
	- 추후 배포를 위해 최적 모델을 BentoML로 저장
5. DAG 내 Task 업데이트
	- 데이터 전처리 때와 유사하게 이미지를 빌드하고 컨테이너를 띄우는 명령어로 구성된 Task 업데이트
	- 일부 구성 업데이트

### CatBoost

![](https://i.imgur.com/9vyXbyF.png)
- 범주형 피처 처리에 강력한 성능을 보이는 그라디언트 부스팅(gradient boosting) 기법
- 특징
	- 범주형 피처 처리
		- 자체적인 **Ordererd Boosting** 기법과 **Target Statistics(TS)** 알고리즘을 통해 범주형 피처를 정교하게 처리함
		- 데이터 누출(Data leakage)를 방지하고 과적합을 방지
	- 하이퍼파라미터 튜닝 최소화
		- 다른 GBM 계열 알고리즘(XGBoost, LightGBM 등)에 비해 하이퍼파라미터 튜닝 없이 기본값으로도 좋은 성능을 보임
	- 텍스트 피처 처리
		- 자체적으로 텍스트 피처를 처리하는 알고리즘을 내장하고 있음


```python
# pipelines/continuous_training/training/trainer.py

import argparse
import os
import shutil
from dataclasses import dataclass
from datetime import datetime
from itertools import product
from typing import Dict, List, Tuple

import bentoml
import mlflow
import numpy.typing as npt
import pandas as pd
from catboost import CatBoostClassifier, Pool
from dotenv import load_dotenv
from mlflow.entities import Run
from mlflow.models.signature import infer_signature
from tqdm.auto import tqdm

from utils.dates import DateValues

# .env 파일에서 환경 변수 로드
load_dotenv()


@dataclass
class TrainingConfig:
    """학습 파이프라인에 필요한 모든 설정을 관리하는 데이터 클래스."""

    model_name: str
    base_dt: str
    artifacts_path: str = os.getenv("ARTIFACTS_PATH", "")
    target_name: str = "credit_score"
    drop_cols: List[str] = list(("base_dt", "id", "customer_id", "date"))
    text_cols: List[str] = list(("type_of_loan", "payment_behaviour"))
    categorical_cols: List[str] = list(
        ("occupation", "credit_mix", "payment_of_min_amount")
    )
    params_candidates: Dict[str, List] = dict(
        {
            "depth": [7, 8, 9],
            "rsm": [0.8, 0.9, 1.0],
            "l2_leaf_reg": [3, 5, 7],
        }
    )


class Trainer:
    """모델 학습 파이프라인을 관리하고 실행하는 클래스."""

    def __init__(self, config: TrainingConfig):
        """
        Trainer 클래스를 초기화합니다.

        Args:
            config (TrainingConfig): 학습 설정 객체
        """
        self._config = config
        self._preprocessing_path = os.path.join(
            self._config.artifacts_path,
            "preprocessing",
            self._config.model_name,
            self._config.base_dt,
        )
        self._model_path = os.path.join(
            self._config.artifacts_path,
            "models",
            self._config.model_name,
            self._config.base_dt,
        )
        self.is_trained = False

    def run(self) -> None:
        """전체 학습 파이프라인을 실행합니다."""
        self._setup_environment()
        train_pool, val_pool, x_train = self._prepare_data()
        experiment_id = self._tune_hyperparameters(
            train_pool, val_pool, x_train
        )
        best_run = self._get_best_run(experiment_id)
        self._save_model_to_bentoml(best_run)
        print(f"모델 학습 및 저장 완료: {self._config.model_name}")

    def _setup_environment(self) -> None:
        """학습 환경을 설정합니다. 기존 모델 경로가 존재하면 제거하고, 새로운 경로를 생성합니다."""
        if os.path.exists(self._model_path):
            shutil.rmtree(self._model_path)
        os.makedirs(self._model_path)
        print(f"학습 환경 설정 완료: {self._model_path}")

    def _prepare_data(self) -> Tuple[Pool, Pool, pd.DataFrame]:
        """
        데이터를 로드하고 CatBoost 학습에 사용할 Pool 객체를 생성합니다.

        Returns:
            Tuple[Pool, Pool, pd.DataFrame]: 학습용 Pool, 검증용 Pool, 학습 피처 데이터프레임
        """
        train_df = pd.read_csv(
            os.path.join(
                self._preprocessing_path, f"{self._config.model_name}_train.csv"
            )
        )
        val_df = pd.read_csv(
            os.path.join(
                self._preprocessing_path, f"{self._config.model_name}_val.csv"
            )
        )

        x_train = train_df.drop(
            [self._config.target_name] + self._config.drop_cols, axis=1
        )
        y_train = train_df[self._config.target_name].to_numpy()
        x_val = val_df.drop(
            [self._config.target_name] + self._config.drop_cols, axis=1
        )
        y_val = val_df[self._config.target_name].to_numpy()

        train_pool = self._create_pool(x_train, y_train)
        val_pool = self._create_pool(x_val, y_val)

        print("데이터 준비 완료.")
        return train_pool, val_pool, x_train

    def _create_pool(self, x: pd.DataFrame, y: npt.NDArray) -> Pool:
        """CatBoost Pool 객체를 생성합니다."""
        return Pool(
            data=x,
            label=y,
            cat_features=self._config.categorical_cols,
            text_features=self._config.text_cols,
        )

    def _tune_hyperparameters(
        self, train_pool: Pool, val_pool: Pool, x_train: pd.DataFrame
    ) -> str:
        """
        하이퍼파라미터 튜닝을 수행하고 최적의 모델을 찾습니다.

        Args:
            train_pool (Pool): 학습용 데이터 Pool
            val_pool (Pool): 검증용 데이터 Pool
            x_train (pd.DataFrame): 모델 서명 추론에 사용할 학습 피처

        Returns:
            str: 생성된 MLflow 실험 ID
        """
        # 파이프라인 실행 시마다 고유한 실험 이름을 생성하여 실행 결과를 명확히 구분합니다.
        # 이를 통해 동일한 모델에 대한 다른 학습 시도들을 쉽게 비교하고 추적할 수 있습니다.
        experiment_name = (
            f"training-{datetime.now().strftime('%Y-%m-%d-%H%M%S')}"
        )
        experiment = mlflow.set_experiment(experiment_name)

        param_set = self._get_params_set(self._config.params_candidates)
        catboost_static_params = self._get_static_params()

        print(f"하이퍼파라미터 튜닝 시작. 실험 이름: {experiment_name}")
        for i, params in enumerate(
            tqdm(param_set, desc="Hyperparameter Tuning")
        ):
            run_name = f"Run {i}"
            with mlflow.start_run(run_name=run_name):  # type: ignore
                cls = CatBoostClassifier(**params, **catboost_static_params)
                cls.fit(
                    train_pool,
                    eval_set=val_pool,
                    early_stopping_rounds=50,
                )

                self._log_to_mlflow(cls, params, x_train)

        self.is_trained = True
        print("하이퍼파라미터 튜닝 완료.")
        return experiment.experiment_id

    def _log_to_mlflow(
        self, model: CatBoostClassifier, params: dict, x_train: pd.DataFrame
    ) -> None:
        """학습된 모델의 정보와 결과를 MLflow에 로깅합니다."""
        mlflow.set_tag("estimator_name", model.__class__.__name__)  # type: ignore
        mlflow.log_params({key: model.get_params()[key] for key in params})  # type: ignore
        mlflow.log_param("iterations", model.best_iteration_)  # type: ignore
        mlflow.log_metrics(  # type: ignore
            self._parse_score_dict(model.get_best_score().get("validation"))
        )

        # `infer_signature`는 모델의 입력 및 출력 스키마를 자동으로 추론합니다.
        # 이는 모델을 사용할 때 데이터 유효성 검사를 활성화하고,
        # MLflow UI에서 모델의 입출력 형식을 명확하게 보여주는 중요한 역할을 합니다.
        mlflow.catboost.log_model(
            model,
            "CatBoostClassifier",
            signature=infer_signature(x_train, model.predict(x_train)),
        )

    def _get_best_run(self, experiment_id: str) -> Run:
        """
        지정된 실험에서 가장 성능이 좋은 실행(Run) 정보를 반환합니다.

        Args:
            experiment_id (str): MLflow 실험 ID

        Raises:
            AttributeError: 학습이 진행되지 않았거나 실험 정보를 찾을 수 없는 경우

        Returns:
            Run: 가장 성능이 좋은 모델의 실행 정보
        """
        if not self.is_trained:
            raise AttributeError(
                "학습이 진행되지 않았습니다. 실험 결과를 가져올 수 없습니다."
            )

        # 현재 비즈니스 요구사항에서는 모델의 '정확도(Accuracy)'를 가장 중요한 성능 지표로 판단합니다.
        # 따라서 Accuracy를 기준으로 최적의 모델을 선택합니다.
        best_run_df = mlflow.search_runs(  # type: ignore
            experiment_ids=[experiment_id],
            order_by=["metrics.Accuracy DESC"],
            max_results=1,
        )

        if len(best_run_df) == 0:
            raise AttributeError(
                f"실험 '{experiment_id}'에서 실행 정보를 찾을 수 없습니다."
            )

        # Ensure best_run_df is a DataFrame and access the first run_id correctly
        if isinstance(best_run_df, pd.DataFrame):
            best_run_id = best_run_df.iloc[0]["run_id"]
        else:
            # Fallback for unexpected types
            best_run_id = best_run_df[0].info.run_id
        print(f"최적 실행 찾음: {best_run_id}")
        return mlflow.get_run(best_run_id)  # type: ignore

    def _save_model_to_bentoml(self, best_run: Run) -> None:
        """최적 모델을 BentoML 형식으로 저장합니다."""
        run_id = best_run.info.run_id
        model_uri = f"runs:/{run_id}/CatBoostClassifier"
        model = mlflow.catboost.load_model(model_uri)

        # BentoML로 모델을 저장할 때 `signatures`를 정의하면,
        # API 서버에서 이 모델의 `predict` 함수를 어떻게 호출할지 명시할 수 있습니다.
        # `batchable=True`는 여러 입력을 한 번에 처리할 수 있도록 하여 처리량을 높입니다.
        # `metadata`에는 모델 학습에 사용된 파라미터를 저장하여, 나중에 모델을 추적하고 재현하는 데 사용합니다.
        bentoml.catboost.save_model(
            name=self._config.model_name,
            model=model,
            signatures={"predict": {"batchable": True, "batch_dim": 0}},
            metadata=best_run.data.params,
        )
        print(f"최적 모델을 BentoML에 저장 완료: {self._config.model_name}")

    @staticmethod
    def _get_params_set(params: dict) -> list:
        """하이퍼파라미터 후보 딕셔너리를 모든 조합의 리스트로 변환합니다."""
        # `itertools.product`를 사용하여 파라미터 후보들의 데카르트 곱(Cartesian product)을 생성합니다.
        # 이를 통해 모든 하이퍼파라미터 조합에 대한 탐색을 자동화할 수 있습니다.
        keys, values = zip(
            *[(k, v if isinstance(v, list) else [v]) for k, v in params.items()]
        )
        return [dict(zip(keys, v)) for v in product(*values)]

    @staticmethod
    def _parse_score_dict(score_dict: dict) -> dict:
        """MLflow 로깅을 위해 CatBoost 점수 딕셔너리의 키를 변환합니다."""
        # MLflow는 메트릭 키에 '=' 문자를 허용하지 않으므로,
        # CatBoost에서 생성된 'F1:class=X'와 같은 키를 'F1:class X'로 변경합니다.
        return {k.replace("=", " "): v for k, v in score_dict.items()}

    @staticmethod
    def _get_static_params() -> dict:
        """CatBoost 학습을 위한 정적 파라미터를 반환합니다."""
        return {
            "loss_function": "MultiClassOneVsAll",
            "learning_rate": 0.3,
            "iterations": 2000,
            "thread_count": -1,
            "random_seed": 42,
            "verbose": 50,
            "custom_metric": ["F1", "Accuracy"],
            # `text_processing`은 CatBoost가 텍스트 특성을 내부적으로 처리하는 방법을 정의합니다.
            # 별도의 TF-IDF나 임베딩 전처리 없이, 모델이 직접 텍스트의 의미를 학습하도록 돕습니다.
            # 여기서는 공백, 쉼표, 밑줄을 기준으로 단어를 분리(토큰화)하고,
            # Bi-gram과 Word 사전을 구축하여 텍스트 특성을 분석합니다.
            "text_processing": {
                "tokenizers": [
                    {
                        "tokenizer_id": "Space",
                        "separator_type": "ByDelimiter",
                        "delimiter": " ",
                    },
                    {
                        "tokenizer_id": "Comma",
                        "separator_type": "ByDelimiter",
                        "delimiter": ",",
                    },
                    {
                        "tokenizer_id": "Underscore",
                        "separator_type": "ByDelimiter",
                        "delimiter": "_",
                    },
                ],
                "dictionaries": [
                    {
                        "dictionary_id": "BiGram",
                        "occurence_lower_bound": 1,
                    },
                    {
                        "dictionary_id": "Word",
                        "occurence_lower_bound": 1,
                    },
                ],
            },
        }


def main():
    """스크립트의 메인 실행 함수."""
    parser = argparse.ArgumentParser(
        description="모델 학습 파이프라인을 위한 인자 파서"
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="credit_score_classification",
        help="학습할 모델의 이름",
    )
    parser.add_argument(
        "--base_dt",
        type=str,
        default=DateValues.get_current_date(),
        help="데이터의 기준 날짜 (YYYY-MM-DD 형식)",
    )
    args = parser.parse_args()

    config = TrainingConfig(model_name=args.model_name, base_dt=args.base_dt)
    trainer = Trainer(config)
    trainer.run()


if __name__ == "__main__":
    main()

```

### Docker 설정파일 개발

- `docker-compose.yml` 파일에 다음과 같이 볼륨을 추가 설정
	- `${HOME}/bentoml:/home/mlops/bentoml`
		- 컨테이너 내에서 `/home/mlops/bentoml`에 저장될 모델을 로컬에서 사용하도록 함
- `Dockerfile`에 학습 관련 폴더를 복사하는 명령 추가
	- `ARG TRAINING_PATH=pipelines/continuous_training/training`
	- `COPY --chown=mlops:mlops ${TRAINING_PATH}/trainer.py ${USER_HOME}/training/`

```dockerfile
# pipelines/continuous_training/docker/Dockerfile

FROM python:3.12-slim-bookworm
LABEL maintainer="otzslayer@gmail.com"
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

ARG USER_HOME=/home/codespace
ARG UTIL_PATH=utils
ARG PREPROCESSING_PATH=pipelines/continuous_training/data_preprocessing
ARG REQUIREMENTS_PATH=pipelines/continuous_training/docker
ARG TRAINING_PATH=pipelines/continuous_training/training

RUN groupadd --gid 1000 codespace \
    && useradd --uid 1000 --gid codespace --shell /bin/bash --create-home codespace

COPY --chown=codespace:codespace ${UTIL_PATH}/ ${USER_HOME}/utils
COPY --chown=codespace:codespace ${PREPROCESSING_PATH}/preprocessor.py \
    ${USER_HOME}/data_preprocessing/
COPY --chown=codespace:codespace ${TRAINING_PATH}/trainer.py \
    ${USER_HOME}/training/
COPY --chown=codespace:codespace ${REQUIREMENTS_PATH}/requirements.txt \
    ${USER_HOME}/

USER codespace
WORKDIR ${USER_HOME}

ENV PYTHONUNBUFFERED=1 \
    VIRTUAL_ENV="${USER_HOME}/.venv"

ENV PATH="${VIRTUAL_ENV}/bin:$PATH"

RUN mkdir -p ${USER_HOME}/artifacts \
    && uv init --python 3.12 \
    && uv venv --python 3.12 --seed \
    && uv pip install -r ${USER_HOME}/requirements.txt
```

```yaml
# pipelines/continuous_training/docker/docker-compose.yml

services:
  continuous_training_pipeline:
    build:
      context: ../../..
      dockerfile: pipelines/continuous_training/docker/Dockerfile
    image: credit_score_classification:ct-pipeline-latest
    container_name: credit_score_classification_ct_pipeline
    volumes:
      - ${HOME}/airflow/artifacts:/home/codespace/artifacts
      - ${HOME}/bentoml:/home/codespace/bentoml
      - ${HOME}/mlruns:/home/codespace/mlruns
    environment:
      PYTHONPATH: /home/codespace
      ARTIFACTS_PATH: /home/codespace/artifacts
      FEATURE_STORE_URL: mysql+pymysql://root:root@mariadb:3306/mlops
    command: >
      python ${PYTHON_FILE} --model_name ${MODEL_NAME} --base_dt ${BASE_DT}
    networks:
      mlops_network:
networks:
  mlops_network:
    name: mlops_network
    external: true

```

### DAG 개발

```python
# pipelines/continuous_training/continuous_training_dag.py

...

training = BashOperator(
	task_id="model_training",
	bash_command=f"cd {airflow_dags_path}/pipelines/continuous_training/docker &&"
	"docker compose up --build && docker compose down",
	env={
		"PYTHON_FILE": "/home/codespace/training/trainer.py",
		"MODEL_NAME": "credit_score_classification",
		"BASE_DT": kst_ds_template,
	},
	append_env=True,
	retries=1,
)

...

```

> [!tip]
> - 학습이 빨리 끝났다면 오류가 난게 아닐지 확인 필요

- DAG 실행 후 Codespace에서 `~/bentoml/models` 에 올바르게 모델이 저장되었는지 확인
- 만약 오류가 발생하였다면
	- `~/airflow/artifacts` 와 `~/bentoml` 가 생성되어 있지 않아 Airflow가 `root` 권한으로 해당 폴더를 생성하면서 권한 문제가 발생했을 가능성 있음
	- 테스트 용도로 먼저 내부에서 코드를 돌려봤거나, 미리 폴더를 생성했다면 문제가 발생하지 않음
- 이미 에러가 발생해서 폴더를 새로 생성하기 꺼려진다면?
	- `sudo chown codespace:codespace -R ~/bentoml`
	- `sudo chown codespace:codespace -R ~/airflow/artifacts`