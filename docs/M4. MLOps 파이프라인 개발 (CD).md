# M4. MLOps 파이프라인 개발 (CD)

## 4.1 모델 배포 (API 개발)

### API 개발 요소

1. 데이터 모델 개발
	- 추론 결과를 저장할 테이블과 관련된 테이블 모델 개발
	- API 입력값과 출력값의 데이터 모델 개발
2. 아티팩트 불러오기
	- 저장한 CatBoost 모델과 전처리 시 저장한 `RobustScaler` 불러오기
3. 모델 추론
	- 입력 받은 데이터로 추론 수행하여 결과 레이블과 그 확률값 반환
4. 로그 데이터 적재
	- 입력 받은 데이터, 결과 레이블, 확률값, 수행 시간 등을 생성해놓은 테이블에 적재
5. 컨테이너 개발
	- API 서비스를 띄워놓기 위한 Docker 컨테이너 개발
	- DAG 개발은 진행하지 않음

### BentoML (Remind)

![](https://i.imgur.com/VvfI2Py.png)

- **ML 모델 서빙만을 위한** 라이브러리
	- 대부분의 **메이저 ML 모델 라이브러리 지원**
	- 대부분의 퍼블릭 클라우드에서 사용 가능
	- 최근에는 LLMOps도 지원함
- 코드 기반으로 이후 Airflow 등 오케스트레이션 도구를 이용하여 Task로 만들 수 있음
- 배치 추론과 실시간 추론 모두 지원함
- 웹 대시보드로 모델 관리나 API 관리 가능
- 코드 몇 줄과 커맨드 몇 줄로 손쉽게 서빙 API 구축 가능

### API 폴더 구조

![](https://i.imgur.com/jPYb9ma.png)

### SQLAlchemy 연동

```python
# api/src/db.py

import os

from dotenv import load_dotenv
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

load_dotenv()

feature_store_url = os.getenv("FEATURE_STORE_URL")

engine = create_engine(feature_store_url, echo=False)
SessionLocal = sessionmaker(
    autocommit=False, autoflush=False, expire_on_commit=False, bind=engine
)
Base = declarative_base()

```

#### 커넥션 풀(Connection Pool)

- 데이터베이스 연결을 미리 여러 개 만들어놓고 필요할 때마다 가져가서 사용하고 반납하는 일종의 커넥션 창고
- 사용하는 이유
	- 데이터베이스에 연결하는 과정은 생각보다 비용이 많이 들어 애플리케이션 성능을 크게 저하함
		- 네트워크 핸드셰이크, 인증 등
	- 연결을 수행하는 시간이 균일하지 않아 안정적인 성능 보장이 어려움
- 작동 원리 → 도서관에서 책을 빌린다고 가정하면
	1. 책 준비 (Pool 생성): 도서관이 미리 여러 권의 인기 도서(DB 커넥션)를 구비해 놓음
	2. 대여 (커넥션 체크아웃): 책을 읽고 싶은 사람(애플리케이션의 요청)이 와서 데스크에 책을 요청하면, 도서관은 이미 있는 책 한 권을 바로 빌려줌 (새로 책을 주문하는 것이 아니라!)
	3. 사용 (Query 실행): 사용자는 빌린 책으로 원하는 작업 수행
	4. 반납 (커넥션 체크인): 작업이 끝나면 책을 도서관에 반납하고, 해당 책은 다른 사람이 바로 빌려갈 수 있음
	5. 오버플로우 (Overflow): 모든 책이 대출 중인데 다른 사람이 책을 빌리러 오면 잠시 기다리거나, 추가로 구비해 둔 여분의 책을 빌려줌
- 사용 이점
	- **성능 향상:** 연결 생성/종료에 드는 시간을 없애 애플리케이션의 응답 속도가 눈에 띄게 빨라집니다.
	- **자원 효율성:** 정해진 수의 연결만 사용하므로 데이터베이스 서버의 부하를 예측하고 관리할 수 있습니다.
	- **안정성:** 커넥션 풀은 오래되어 끊어진 연결(Stale Connection)을 자동으로 감지하고 새로운 연결로 교체하는 등 연결을 안정적으로 관리해 줍니다. (`pool_recycle` 옵션)

#### `sessionmaker`

- SQLAlchemy에서 **데이터베이스 세션을 생성하는 팩토리**
	- **DB와의 연결을 관리하는 세션을 쉽게 만들고 사용할 수 있도록 도와주는 도구**

#### `sessionmaker`의 필요성

1. 요청마다 독립적인 작업 공간 제공해 **독립성 및 스레드 안정성**을 제공
	- 각각의 API 요청이 들어올 때마다 `sessionmaker`를 호출해 별도의 세션을 생성
	- 각 사용자의 요청은 각자의 세션에서만 처리되므로 스레드에 안전(thread-safe)한 작업
2. 일관된 세션 생성
	- `sessionmaker`를 사용하면 한 곳에서 설정을 관리하여 동일한 설정을 가진 세션을 반복해서 쉽게 만들 수 있음
3. 트랜잭션 관리 용이
	- 세션을 `sessionmaker`를 통해 생성한 경우 `with session.begin()` 절을 통해 명확한 구조로 트랜잭션을 관리할 수 있음


### 테이블 모델

```python
# api/src/models.py

from sqlalchemy import JSON, Column, DateTime, Float, Integer, String, func

from api.src.db import Base


class CreditPredictionApiLog(Base):
    __tablename__ = "credit_predictions_api_log"

    id = Column(Integer, primary_key=True, autoincrement=True)
    customer_id = Column(String(10), nullable=False)
    features = Column(JSON, nullable=False)
    prediction = Column(String(10), nullable=False)
    confidence = Column(Float, nullable=False)
    elapsed_ms = Column(Integer, nullable=False)
    created_at = Column(DateTime, server_default=func.now())

```

- ORM 기능의 활용
- 테이블 이름을 설정하고 아래에 각 컬럼의 이름에 대해 알맞는 데이터 타입, PK 여부, auto increment 여부, NOT NULL 여부 등을 지정
- 이후 해당 클래스를 이용하여 어렵지 않게 데이터 추가, 삭제 등 가능

### API 스키마

- Pydantic을 활용하여 API에 입출력에 대한 스키마를 작성
	- 값에 대한 검증, 타입 관리가 용이

```python
# api/src/schemas.py

from datetime import datetime
from typing import Any, Dict

from bentoml import IODescriptor
from pydantic import BaseModel, field_validator


class Features(BaseModel):
    customer_id: int
    age: int
    occupation: str
    annual_income: float
    monthly_inhand_salary: float
    num_bank_accounts: float
    num_credit_card: float
    interest_rate: float
    num_of_loan: int
    type_of_loan: str
    delay_from_due_date: float
    num_of_delayed_payment: float
    changed_credit_limit: float
    num_credit_inquiries: float
    credit_mix: str
    outstanding_debt: float
    credit_utilization_ratio: float
    credit_history_age: float
    payment_of_min_amount: str
    total_emi_per_month: float
    amount_invested_monthly: float
    payment_behaviour: str
    monthly_balance: float

    @field_validator("age", "credit_history_age")
    @classmethod
    def validate_age(cls, value):
        if value > 0:
            return value
        raise ValueError("0보다 커야합니다.")

    @field_validator("credit_mix")
    @classmethod
    def validate_credit_mix(cls, value):
        if value in ["Good", "Bad", "Standard"]:
            return value
        raise ValueError(
            "'Good', 'Bad', 'Standard' 중 하나의 값을 가져야 합니다."
        )

    @field_validator("payment_of_min_amount")
    @classmethod
    def validate_payment_of_min_amount(cls, value):
        if value in ["NM", "Yes", "No"]:
            return value
        raise ValueError("'NM', 'Yes', 'No' 중 하나의 값을 가져야 합니다.")


class Response(BaseModel):
    customer_id: int
    predict: str
    confidence: float


class MetadataResponse(IODescriptor):
    model_name: str
    model_version: str
    params: Dict[str, Any]
    creation_time: datetime

```

### 서비스 코드

```python
# api/services.py

import os
import time
import warnings

import bentoml
import joblib
import numpy as np
import pandas as pd
from dotenv import load_dotenv

from api.src.db import SessionLocal
from api.src.models import CreditPredictionApiLog
from api.src.schemas import Features, MetadataResponse, Response
from utils.dates import DateValues

warnings.filterwarnings(action="ignore")

# .env 파일 로드
load_dotenv()

MODEL_NAME = "credit_score_classification"
BASE_DT = DateValues.get_current_date()

artifacts_path = os.getenv("ARTIFACTS_PATH")
encoder_path = os.path.join(
    artifacts_path, "preprocessing", MODEL_NAME, BASE_DT, "encoders"
)


@bentoml.service(
    resources={"cpu": "2"},
    traffic={"timeout": 10},
)
class CreditScoreClassifier:
    """
    신용 점수 분류를 위한 BentoML 서비스입니다.

    이 서비스는 전처리된 데이터를 입력받아 신용 등급을 예측하고,
    예측 로그를 데이터베이스에 기록합니다.
    """

    def __init__(self) -> None:
        """
        서비스를 초기화하고 필요한 모델과 인코더를 로드합니다.
        """
        self.session_maker = None
        self.bento_model = bentoml.models.get("credit_score_classifier:latest")
        self.robust_scalers = joblib.load(
            os.path.join(encoder_path, "robust_scaler.joblib")
        )
        self.model = bentoml.catboost.load_model(self.bento_model)

    @bentoml.on_startup
    def initialize(self):
        self.session_maker = SessionLocal

    @bentoml.api
    def predict(self, data: Features) -> Response:
        """
        입력된 고객 특징(features)을 기반으로 신용 등급을 예측합니다.

        Args:
            data (Features): 예측에 사용할 고객 특징 데이터입니다.
            ctx (Context): 요청 컨텍스트로, DB 세션에 접근하는 데 사용됩니다.

        Returns:
            Response: 예측된 신용 등급과 신뢰도 점수를 포함하는 응답입니다.
        """
        start_time = time.time()
        df = pd.DataFrame([data.model_dump()])
        customer_id = df.pop("customer_id").item()

        for col, scaler in self.robust_scalers.items():
            df[col] = scaler.transform(df[[col]])

        prob = np.max(self.model.predict(df, prediction_type="Probability"))
        label = self.model.predict(df, prediction_type="Class").item()
        elapsed_ms = (time.time() - start_time) * 1000

        record = CreditPredictionApiLog(
            customer_id=customer_id,
            features=data.model_dump(),
            prediction=label,
            confidence=prob,
            elapsed_ms=elapsed_ms,
        )
        with self.session_maker() as db:
            with db.begin():
                db.add(record)

        return Response(customer_id=customer_id, predict=label, confidence=prob)

    @bentoml.api(route="/metadata", output_spec=MetadataResponse)
    def metadata(self) -> MetadataResponse:
        """현재 컨테이너에서 서빙 중인 모델의 메타데이터를 반환합니다."""
        return MetadataResponse(
            model_name=self.bento_model.tag.name,
            model_version=self.bento_model.tag.version,
            params=self.bento_model.info.metadata,
            creation_time=self.bento_model.info.creation_time,
        )


```

- `initialize` 메서드
	- BentoML 서비스 최초 기동 시 실행되는 메서드
	- `sessionmaker`를 설정하여 DB 세션 연결을 가능하도록 함

- DB 세션 연결
	- `with` 컨텍스트 매니저를 이용해 세션을 불러옴
	- `with db.begin()` 을 이용해서 안전한 트랜잭션 관리를 수행
		- `with` 절 내부의 로직에서 에러가 발생하면 커밋하지 않고 롤백
		- `with` 절 내부의 로직이 성공적으로 완료되면 그대로 커밋

#### 부하 테스트

- (MLOps 관점에서) 부하 테스트
	- ML 모델 API가 실제 서비스 환경에서 사용자 요청을 얼마나 잘 처리하는지 확인하고, 성능 한계점을 파악하는 과정
	- API에 최대 사용자 수를 설정하고 초 당 증가 사용자 수를 설정하여 부하 확인
- 부하 테스트가 중요한 이유
	1. ML은 **자원 집약적(computationally intensive)**이기 때문
		- 일반 데이터 조회와 달리 ML 모델 추론은 **CPU, GPU, 메모리 등 시스템 자원을 상대적으로 많이 사용하기 때문**
		- 요청이 조금만 몰려도 시스템 전체가 급격히 느려질 수 있음
	2. 응답 시간의 민감성
		- 추천 시스템과 같은 일부 ML 서비스는 **실시간에 가까운 응답이 필요**
			- 응답 시간의 지연이 사용자의 이탈로 이어질 수 있기 때문
		- 따라서 반드시 부하가 발생했을 때 응답 시간이 얼마나 지연되는지 확인 필요
	3. 예측 불가능한 병목 현상 확인
		- 모델 자체의 연산 속도뿐만 아니라, 데이터 전처리, 네트워크 지연, DB 조회 등 다양한 구간에서 발생할 수 있는 병목을 확인
		- 예를 들어 현재 동기(sync) 연동한 DB를 비동기(async) 처리하면 과연 빨라질까?
			- DB 연동 작업 시간이 길지 않았기에 되려 비동기 작업 자체의 오버헤드가 응답 시간을 늦출 수 있음

- [Locust](https://locust.io/)
	- Python에서 부하 테스트를 쉽게 수행하도록 도와주는 도구

```python
# tests/locust_load_test.py

import random

from locust import HttpUser, between, task


def get_random_features():
    return {
        "customer_id": random.randint(1, 100000),
        "age": random.randint(18, 70),
        "occupation": random.choice(
            [
                "Developer",
                "Journalist",
                "Scientist",
                "Engineer",
                "Architect",
                "Lawyer",
                "Doctor",
                "Manager",
                "Musician",
                "Teacher",
                "Entrepreneur",
                "Mechanic",
                "Writer",
                "Media_Manager",
                "Accountant",
            ]
        ),
        "annual_income": random.uniform(5000, 200000),
        "monthly_inhand_salary": random.uniform(400, 15000),
        "num_bank_accounts": float(random.randint(0, 10)),
        "num_credit_card": float(random.randint(0, 10)),
        "interest_rate": float(random.randint(1, 35)),
        "num_of_loan": random.randint(0, 10),
        "type_of_loan": "Personal Loan",
        "delay_from_due_date": float(random.randint(0, 60)),
        "num_of_delayed_payment": float(random.randint(0, 25)),
        "changed_credit_limit": float(random.uniform(0, 30)),
        "num_credit_inquiries": float(random.randint(0, 15)),
        "credit_mix": random.choice(["Good", "Standard", "Bad"]),
        "outstanding_debt": float(random.uniform(0, 5000)),
        "credit_utilization_ratio": random.uniform(20, 50),
        "credit_history_age": float(random.randint(10, 400)),
        "payment_of_min_amount": random.choice(["Yes", "No", "NM"]),
        "total_emi_per_month": float(random.uniform(0, 1000)),
        "amount_invested_monthly": float(random.uniform(0, 1000)),
        "payment_behaviour": "Low_spent_Small_value_payments",
        "monthly_balance": float(random.uniform(0, 1200)),
    }


class CreditScoreUser(HttpUser):
    wait_time = between(1, 5)

    @task
    def predict(self):
        headers = {"Content-Type": "application/json"}
        features = get_random_features()
        payload = {"data": features}
        self.client.post("/predict", json=payload, headers=headers)

```

- 다음 명령어로 Locust 실행하여 부하테스트 수행
	- 기본 포트가 겹치므로 `8090` 포트 명시
	- API 서버는 미리 띄워놓은 BentoML 서버 URL을 명시

```shell
locust -f tests/locust_load_test.py --host http://localhost:3000 --web-port 8090
```

- 결과 분석

![](https://i.imgur.com/ZKrCdwo.png)
![](https://i.imgur.com/qHs7ko0.png)

- 처리량(Throughput)
	- 사용자가 최대(100명)로 늘어난 후에도 초 당 요청 수(RPS, Requests per second)가 약 32.3에서 변화가 발생하지 않음 → 꾸준히 요청을 처리함
- 응답 시간(Response Times, Latency)
	- 중앙값(Median, 50th percentile)은 76ms로 안정적임 (사용자의 50%는 빠른 응답속도로 사용)
	- 95th percentile은 260ms로 사용자 수가 증가함에 따라 다소 불안정한 느낌이 있음
		- 특정 구간에선 400ms 이상으로 치솟음
	- 중앙값과 95th percentile 사이의 차이에서 병목이 발생할 수 있음을 확인 가능
- 안정성 및 에러 (Stability & Errors)
	- 전체 3,618개 요청 중 4개 실패 → 실패율 0.11%
	- 매우 낮은 실패율이지만 원인 파악이 필요
			-  `RemoteDisconnected('Remote end closed connection without response')`
				- 요청이 서버로 도착하기도 전에 거부되어 발생한 에러로 BentoML 차원에서 확인할 수는 없음
				- 서비스를 CPU 2개로만 띄워서 발생할 확률이 높음
				- Locust로 부하 테스트 시 부하를 더 점진적으로 증가시키는 것도 방법

### Docker 설정 파일 개발

#### Dockerfile

```Dockerfile
# api/docker/Dockerfile

FROM python:3.12-slim-bookworm
LABEL maintainer="otzslayer@gmail.com"
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

ARG USER_HOME=/home/codespace
ARG UTIL_PATH=utils
ARG API_PATH=api

RUN groupadd --gid 1000 codespace \
    && useradd --uid 1000 --gid codespace --shell /bin/bash --create-home codespace

COPY --chown=codespace:codespace ${UTIL_PATH}/ ${USER_HOME}/utils
COPY --chown=codespace:codespace ${API_PATH}/bentofile.yaml ${USER_HOME}/
COPY --chown=codespace:codespace ${API_PATH}/requirements.txt ${USER_HOME}/
COPY --chown=codespace:codespace ${API_PATH}/services.py ${USER_HOME}/
COPY --chown=codespace:codespace ${API_PATH}/src/ ${USER_HOME}/${API_PATH}/src

USER codespace
WORKDIR ${USER_HOME}/${API_PATH}
ENV PYTHONUNBUFFERED=1 \
    VIRTUAL_ENV="${USER_HOME}/${API_PATH}/.venv"

ENV PATH="${VIRTUAL_ENV}/bin:${USER_HOME}/.local/bin:${PATH}"

RUN uv init --python 3.12 \
    && uv venv --python 3.12 --seed \
    && uv pip install -r ${USER_HOME}/requirements.txt

```

#### `docker-compose.yml`

```yml
# api/docker/docker-compose.yml

services:
  bentoml_service:
    build:
      context: ../..
      dockerfile: api/docker/Dockerfile
    image: credit_score_classification-deploy:latest
    container_name: credit_score_classification_deploy
    volumes:
      - ${HOME}/airflow/artifacts:/home/codespace/artifacts
      - ${HOME}/bentoml:/home/codespace/bentoml
    environment:
      PYTHONPATH: /home/codespace
      ARTIFACTS_PATH: /home/codespace/artifacts
      FEATURE_STORE_URL: mysql+pymysql://root:root@mariadb:3306/mlops
    ports:
      - "3000:3000"
    command: >
      bentoml serve services:CreditScoreClassifier
    networks:
      mlops_network:
networks:
  mlops_network:
    name: mlops_network
    external: true

```

## 4.2 지속적 배포 구현

### 지속적 배포 DAG

![](https://i.imgur.com/vkQl8TI.png)

1. API 상태를 체크하여 **API가 작동하지 않을 때는 바로 신규 모델 배포**
2. API가 활성화되어 있는 상태인 경우 **현재 배포되어 있는 모델의 학습일**과 **가장 최근에 학습된 모델의 학습일**을 체크
3. 두 학습일을 다음 Task로 넘겨 비교
4. **배포된 모델보다 학습된 모델이 더 최근에 생성되었거나 배포된 모델이 없다면** 최신 학습 모델로 모델 배포
5. **배포된 모델이 최신이거나 학습된 모델이 없는 경우** 배포 스킵

### 지속적 배포 DAG 개발 

```python
# pipelines/continuous_deployment/continuous_deployment_dag.py
from datetime import datetime

import bentoml
import pendulum
import requests
from airflow.providers.standard.operators.bash import BashOperator
from airflow.providers.standard.operators.python import (
    BranchPythonOperator,
    PythonOperator,
)
from airflow.sdk import DAG, Variable, get_current_context

from utils.callbacks import failure_callback, success_callback

local_timezone = pendulum.timezone("Asia/Seoul")
airflow_dags_path = Variable.get("AIRFLOW_DAGS_PATH")


def get_branch_by_api_status() -> list[str] | str:
    """
    API 상태를 확인하여 분기를 결정하는 함수.
    Airflow 3.x에서는 provide_context가 제거되어 함수 시그니처 변경 불필요.
    """
    try:
        response = requests.get("http://localhost:3000/healthz")
        if response.status_code == 200:
            return [
                "get_deployed_model_creation_time",
                "get_latest_trained_model_creation_time",
            ]
        else:
            return "deploy_new_model"
    except Exception as e:
        print(f"API 통신이 이루어지지 않았습니다.: {e}")
        return "deploy_new_model"


def get_deployed_model_creation_time() -> datetime | None:
    """이미 배포된 모델의 `creation_time`을 조회합니다."""
    try:
        response = requests.post("http://localhost:3000/metadata")
        if response.status_code == 200:
            return datetime.strptime(
                response.json().get("creation_time"), "%Y-%m-%dT%H:%M:%S.%fZ"
            )
        else:
            print(
                f"`creation_time`을 불러올 수 없습니다.: {response.status_code}"
            )
            return None
    except Exception as e:
        print(f"배포된 모델의 API를 받아오지 못했습니다.: {e}")
        return None


def get_latest_trained_model_creation_time() -> datetime | None:
    """로컬 저장소에 저장된 최신 학습 모델의 `creation_time` 조회합니다."""
    try:
        bento_model = bentoml.models.get("credit_score_classifier:latest")
        return bento_model.info.creation_time.replace(tzinfo=None)
    except Exception as e:
        print(f"Error getting latest trained model creation time: {e}")
        return None


def decide_model_update():
    """
    현재 배포된 모델과 로컬 최신 학습 모델의 creation_time 비교.
    배포된 모델이 오래되었으면 새로운 모델을 배포하도록 결정.

    Airflow 3.x에서는 명시적으로 ti를 인자로 받는 것을 권장하지만,
    기존 코드와의 호환성을 위해 이 방식도 계속 지원.
    """
    context = get_current_context()
    ti = context["ti"]
    api_status = ti.xcom_pull(task_ids="get_branch_by_api_status")

    if api_status == "deploy_new_model":
        return "deploy_new_model"

    deployed_creation_time = ti.xcom_pull(
        task_ids="get_deployed_model_creation_time"
    )
    trained_creation_time = ti.xcom_pull(
        task_ids="get_latest_trained_model_creation_time"
    )

    print("deployed_creation_time", deployed_creation_time)
    print("trained_creation_time", trained_creation_time)

    if deployed_creation_time is None:
        print("There is no deployed model!")
        return "deploy_new_model"

    if (
        trained_creation_time is not None
        and trained_creation_time > deployed_creation_time
    ):
        print("Deployed model is already out-of-date.")
        return "deploy_new_model"

    print("Skip deployment.")
    return "skip_deployment"


with DAG(
    dag_id="credit_score_classification_cd",
    default_args={
        "owner": "user",
        "depends_on_past": False,
        "email": ["otzslayer@gmail.com"],
        "on_failure_callback": failure_callback,
        "on_success_callback": success_callback,
    },
    description="A DAG for continuous deployment",
    schedule=None,
    start_date=datetime(2025, 1, 1, tzinfo=local_timezone),
    catchup=False,
    tags=set(["lgcns", "mlops"]),
) as dag:
    # API 상태 체크 결과 가져오기
    get_api_status_task = BranchPythonOperator(
        task_id="get_branch_by_api_status",
        python_callable=get_branch_by_api_status,
    )

    # 현재 컨테이너에서 실행 중인 모델의 creation_time 가져오기
    get_deployed_model_creation_time_task = PythonOperator(
        task_id="get_deployed_model_creation_time",
        python_callable=get_deployed_model_creation_time,
    )

    # 로컬에서 최신 학습된 모델의 creation_time 가져오기
    get_latest_trained_model_creation_time_task = PythonOperator(
        task_id="get_latest_trained_model_creation_time",
        python_callable=get_latest_trained_model_creation_time,
    )

    # 모델을 업데이트할지 결정
    # provide_context=True 매개변수 제거됨
    decide_update_task = BranchPythonOperator(
        task_id="decide_update",
        python_callable=decide_model_update,
    )

    # 새로운 모델을 배포
    deploy_new_model_task = BashOperator(
        task_id="deploy_new_model",
        bash_command=f"cd {airflow_dags_path}/api/docker &&"
        "docker compose up --build --detach",
        trigger_rule="one_success",
    )

    # 배포를 건너뛸 경우 실행할 더미 태스크
    skip_deployment_task = PythonOperator(
        task_id="skip_deployment",
        python_callable=lambda: print("No new model to deploy"),
    )

    # DAG 실행 순서 정의
    # 1️⃣ API가 정상 동작하지 않으면 즉시 배포
    get_api_status_task >> deploy_new_model_task

    # 2️⃣ API가 정상 동작하면 모델 생성 시간 비교 후 업데이트 결정
    (
        get_api_status_task
        >> [
            get_deployed_model_creation_time_task,
            get_latest_trained_model_creation_time_task,
        ]
        >> decide_update_task
    )

    # 3️⃣ decide_update_task의 결과에 따라 모델 배포 여부 결정
    decide_update_task >> [deploy_new_model_task, skip_deployment_task]

```

### HTTP 응답 상태 코드 


| 코드 | 의미                  | 설명                                                                    | 예시                                       |
| ---- | --------------------- | ----------------------------------------------------------------------- | ------------------------------------------ |
| 200  | OK                    | 요청이 성공적으로 처리됨 (일반적인 성공 응답)                           | 데이터 조회 성공                           |
| 201  | Created               | 요청이 성공적으로 처리되었으며, 새로운 리소스가 생성됨 (POST 요청 결과) | 데이터 생성 성공                           |
| 400  | Bad Request           | 클라이언트의 요청이 잘못됨 (유효하지 않은 데이터, 형식 오류 등)         | 잘못된 요청 (폼 입력 오류)                 |
| 401  | Unauthorized          | 인증이 필요함                                                           | 인증 필요 (로그인 필요)                    |
| 403  | Forbidden             | 접근이 금지됨 (권한 부족)                                               | 권한 부족 (접근 거부)                      |
| 404  | Not Found             | 요청한 리소스를 찾을 수 없음                                            | 리소스 없음 (잘못된 URL)                   |
| 500  | Internal Server Error | 서버 내부 오류 발생                                                     | 서버 오류 발생                             |
| 502  | Bad Gateway           | 서버가 게이트웨이 역할을 하며 다른 서버로부터 잘못된 응답을 받음        | 백엔드 서버의 잘못된 응답                  |
| 503  | Service Unavailable   | 서버가 과부하 상태이거나 유지보수 중이라 사용할 수 없음                 | 과부하나 자원 부족으로 인한 서버 응답 불가 |

### `PythonOperator` vs `BranchPythonOperator`

#### `PythonOperator`

- 역할
	- 일반적인 Python 함수를 실행하는 Operator
	- DAG 내에서 특정 Python 함수를 실행하고 결과 반환
- 특징
	- 단순히 지정된 Python 함수를 실행
	- 실행 결과를 사용할 수도 있고 사용하지 않을 수도 있음
	- **DAG의 흐름을 바꾸지 않음**

#### `BranchPythonOperator`

- 역할
	- 특정 조건에 따라 DAG의 실행 흐름을 **분기(branching)**할 수 있는 Operator
	- Python 함수의 반환 값이 실행할 다음 Task를 결정함
- 특징
	- DAG의 실행 흐름을 동적으로 변경할 수 있음
	- **하나 이상의 Task ID를 반환**해야 함
	- 선택된 Task만 실행되며, 선택되지 않은 Task는 Skipped 상태가 됨
